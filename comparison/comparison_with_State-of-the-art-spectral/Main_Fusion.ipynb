{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Main_Fusion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TvF0CndBFs1m"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGdsJ0pNojQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaXUgbTnNv-v"
      },
      "source": [
        "# Libraries\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_gmgfEWXD9w"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from os import listdir\n",
        "\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import keras\n",
        "import scipy.io\n",
        "from scipy.io import loadmat\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from keras import backend as K\n",
        "from os import listdir\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.constraints import MinMaxNorm,NonNeg\n",
        "import scipy \n",
        "from scipy import interpolate\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "%cd /content/drive/My Drive/DeepFusion\n",
        "from Read_Data_set import *\n",
        "from Filter_pattern import *\n",
        "!nvidia-smi\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VXvUBgOB4k"
      },
      "source": [
        "# Patterned Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3m77PGJhPHM"
      },
      "source": [
        "def get_color_bases(wls):\n",
        "    SG = [0.0875,0.1098,0.1157,0.1245,0.1379,0.1561,0.1840,0.2458,0.3101,0.3384,0.3917\\\n",
        "        ,0.5000,0.5732,0.6547,0.6627,0.624,0.5719,0.5157,0.4310,0.3470,0.2670,0.1760\\\n",
        "        ,0.1170,0.0874,0.0754,0.0674,0.0667,0.0694,0.0567,0.0360,0.0213]   # green color\n",
        "\n",
        "    SB = [0.2340,0.2885,0.4613,0.5091,0.5558,0.5740,0.6120,0.6066,0.5759,0.4997\\\n",
        "        ,0.4000,0.3000,0.2070,0.1360,0.0921,0.0637,0.0360,0.0205,0.0130,0.0110\\\n",
        "        ,0.0080,0.0060,0.0062,0.0084,0.0101,0.0121,0.0180,0.0215,0.0164,0.0085\\\n",
        "        ,0.0050]                                                           # blue color\n",
        "\n",
        "    SR = [0.1020,0.1020,0.0790,0.0590,0.0460,0.0360,0.0297,0.0293,0.0310,0.03230\\\n",
        "          ,0.0317,0.0367,0.0483,0.0667,0.0580,0.0346,0.0263,0.0487,0.1716,0.4342\\\n",
        "          ,0.5736,0.5839,0.5679,0.5438,0.5318,0.5010,0.4810,0.4249,0.2979,0.1362\\\n",
        "          ,0.0651]                                                         # red color\n",
        "\n",
        "    SC = [0.1895,0.2118,0.1947,0.1835,0.1839,0.1921,0.2137,0.2751,0.3411,0.3707\\\n",
        "        ,0.4234,0.5367,0.6215,0.7214,0.7207,0.6586,0.5982,0.5644,0.6026,0.7812\\\n",
        "        ,0.8406,0.7599,0.6849,0.6312,0.6072,0.5684,0.5477,0.4943,0.3546,0.1722\\\n",
        "        ,0.0864]                                                          # cyan color\n",
        "\n",
        "    x_wvls = np.linspace(399e-9,701e-9,len(SG))\n",
        "\n",
        "    fg = interpolate.interp1d(x_wvls, SG)\n",
        "    fr = interpolate.interp1d(x_wvls, SR)\n",
        "    fc = interpolate.interp1d(x_wvls, SC)\n",
        "    fb = interpolate.interp1d(x_wvls, SB)\n",
        "                                                                                                                               \n",
        "    fr = fr(wls)\n",
        "    fg = fg(wls)\n",
        "    fc = fc(wls)\n",
        "    fb = fb(wls)\n",
        "\n",
        "    fr = tf.convert_to_tensor(fr, dtype=tf.float32)\n",
        "    fg = tf.convert_to_tensor(fg, dtype=tf.float32)\n",
        "    fc = tf.convert_to_tensor(fc, dtype=tf.float32)\n",
        "    fb = tf.convert_to_tensor(fb, dtype=tf.float32)\n",
        "\n",
        "    fr = tf.expand_dims(tf.expand_dims(fr, 0), 0)\n",
        "    fg = tf.expand_dims(tf.expand_dims(fg, 0), 0)\n",
        "    fc = tf.expand_dims(tf.expand_dims(fc, 0), 0)\n",
        "    fb = tf.expand_dims(tf.expand_dims(fb, 0), 0)\n",
        "\n",
        "\n",
        "    return fr,fg,fc,fb\n",
        "\n",
        "def multispectral_Im(x):\n",
        "  ds = x[3]\n",
        "  dm = x[2]\n",
        "  filters = x[1]\n",
        "  x = x[0]\n",
        "\n",
        "  print('Original Filters Shape: ', filters.shape)\n",
        "\n",
        "  filters = tf.keras.backend.repeat_elements(filters, rep=ceil(1/dm), axis=1)\n",
        "  filters = tf.keras.backend.repeat_elements(filters, rep=ceil(1/dm), axis=0)\n",
        "  filters = tf.image.resize(filters,[x.shape[1],x.shape[2]])\n",
        "  print('Replicated Filter Shape: ', filters.shape)\n",
        "  filters = tf.broadcast_to(filters, shape=tf.shape(x))\n",
        "  output = tf.expand_dims(tf.math.reduce_sum(tf.multiply(x, filters), axis=-1),-1)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class MultispectralAcq(Layer):\n",
        "    def __init__(self, output_dim=(256, 256, 12), input_dim=(256, 256, 12), w_ini=400, w_final=700, ds = 1, dm = 1/9, sd = 0.5, train = True,**kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.M = input_dim[0]\n",
        "        self.N = input_dim[1]\n",
        "        self.L = input_dim[2]\n",
        "        self.Lp = ceil(input_dim[2]*sd)\n",
        "        self.ds = ds\n",
        "        self.dm = dm\n",
        "        self.sd = sd\n",
        "        self.train = train\n",
        "\n",
        "        fr,fg,fc,fb=get_color_bases(np.linspace(w_ini, w_final, ceil(input_dim[2]*sd))*1e-9)\n",
        "\n",
        "        self.fr = tf.expand_dims(fr, 0)\n",
        "        self.fg = tf.expand_dims(fg, 0)\n",
        "        self.fc = tf.expand_dims(fc, 0)\n",
        "        self.fb = tf.expand_dims(fb, 0)\n",
        "\n",
        "\n",
        "        super(MultispectralAcq, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'output_dim': self.output_dim,\n",
        "            'input_dim': self.input_dim,\n",
        "            'fr': self.fr,\n",
        "            'fg': self.fg,\n",
        "            'fc': self.fc,\n",
        "            'fb': self.fb,\n",
        "            'ds': self.ds,\n",
        "            'dm': self.dm,\n",
        "            'train': slef.train})\n",
        "      \n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        sd = self.sd\n",
        "        train =self.train\n",
        "        M = self.M\n",
        "        N = self.N\n",
        "        Lp = self.Lp\n",
        "        dm = self.dm\n",
        "        Mm = int(M*dm)\n",
        "        Nm = int(N*dm)\n",
        "        wr = (np.random.rand(Mm, Nm, 1))\n",
        "        wg = (np.random.rand(Mm, Nm, 1))\n",
        "        wb = (np.random.rand(Mm, Nm, 1))\n",
        "        wc = (np.random.rand(Mm, Nm, 1))\n",
        "        wt = wr + wg + wb + wc\n",
        "        wg = tf.constant_initializer(np.divide(wg, wt))\n",
        "        wb = tf.constant_initializer(np.divide(wb, wt))\n",
        "        wc = tf.constant_initializer(np.divide(wc, wt))\n",
        "        wr = tf.constant_initializer(np.divide(wr, wt))  \n",
        "        print(train)\n",
        "        if train: \n",
        "          print('Trainable MCFA Layer')\n",
        "         \n",
        "          self.wr = self.add_weight(name='wr', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wr, trainable=True, constraint=NonNeg())\n",
        "          self.wg = self.add_weight(name='wg', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wg, trainable=True, constraint=NonNeg())\n",
        "          self.wb = self.add_weight(name='wb', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wb, trainable=True, constraint=NonNeg())\n",
        "          self.wc = self.add_weight(name='wc', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wc, trainable=True, constraint=NonNeg())\n",
        "        else:\n",
        "          print('No Trainable MCFA Layer') \n",
        "\n",
        "          self.wr = self.add_weight(name='wr', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wr, trainable=False)\n",
        "          self.wg = self.add_weight(name='wg', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wg, trainable=False)\n",
        "          self.wb = self.add_weight(name='wb', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wb, trainable=False)\n",
        "          self.wc = self.add_weight(name='wc', shape=(1, Mm, Nm, 1),\n",
        "                                    initializer=wc, trainable=False)\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \n",
        "        \n",
        "\n",
        "        M = self.M\n",
        "        N = self.N\n",
        "        L = self.L\n",
        "        Lp = self.Lp\n",
        "\n",
        "        inputs = tf.expand_dims(tf.reshape(tf.image.resize(tf.expand_dims(tf.reshape(inputs,[M*N,L]),-1),[M*N,Lp]),[M,N,Lp]),0)\n",
        "        \n",
        "        ds = self.ds\n",
        "        dm = self.dm\n",
        "        Mm = int(M*dm)\n",
        "        Nm = int(N*dm)\n",
        "        if self.trainable:\n",
        "          \n",
        "          wt = self.wr + self.wg + self.wb + self.wc\n",
        "          wr = tf.math.divide(self.wr, wt)\n",
        "          wg = tf.math.divide(self.wg, wt)\n",
        "          wb = tf.math.divide(self.wb, wt)\n",
        "          wc = tf.math.divide(self.wc, wt)\n",
        "        else:\n",
        "          \n",
        "          wr = self.wr\n",
        "          wg = self.wg\n",
        "          wb = self.wb\n",
        "          wc = self.wc\n",
        "\n",
        "\n",
        "\n",
        "        H = tf.multiply(wr, self.fr) + tf.multiply(wg, self.fg) + tf.multiply(wb, self.fb) + tf.multiply(wc, self.fc)\n",
        "        \n",
        "\n",
        "        if ds < dm:\n",
        "            y = tf.multiply(H,inputs)\n",
        "            y = tf.image.resize(y,[int(M*ds), int(N*ds)])\n",
        "            y = tf.expand_dims(tf.reduce_sum(y,axis=-1),axis=-1)\n",
        "        elif  ds > dm:\n",
        "\n",
        "            H = tf.keras.backend.repeat_elements(H, rep=int(ceil(1/dm)), axis=2)\n",
        "            H = tf.keras.backend.repeat_elements(H, rep=int(ceil(1/dm)), axis=1)\n",
        "           \n",
        "            H = tf.image.resize(H,[M,N])\n",
        "           \n",
        "            y = tf.multiply(H,inputs) \n",
        "            y = tf.reduce_sum(y,axis=-1)\n",
        "            y = tf.expand_dims(y,axis=-1)\n",
        "            \n",
        "\n",
        "\n",
        "        elif ds == dm:\n",
        "            y = tf.multiply(H,inputs)\n",
        "            y = tf.expand_dims(tf.reduce_sum(y,axis=-1),axis=-1)\n",
        "\n",
        "        #y = tf.expand_dims(y,axis=-1)\n",
        "        X = None\n",
        "        for i in range(Lp):\n",
        "          if X is not None:\n",
        "\n",
        "            X = tf.concat([X,y],-1)\n",
        "          else:\n",
        "            X = y\n",
        "\n",
        "        X = tf.transpose(X,[0,1,2,3])\n",
        "        \n",
        "        \n",
        "        \n",
        "        X = tf.multiply(H, X)\n",
        "\n",
        "\n",
        "        X = X / tf.math.reduce_max(X) \n",
        "            \n",
        "        return X,H,y\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOmbvLbuOJr_"
      },
      "source": [
        "# CASSI Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIWDdZyxPwyy"
      },
      "source": [
        "class Reg_Binary_0_1(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, parameter=10):\n",
        "        self.parameter = tf.keras.backend.variable(parameter,name='parameter')\n",
        "    def __call__(self, x):\n",
        "        regularization = self.parameter*(tf.reduce_sum(tf.multiply(tf.square(x),tf.square(1-x))))\n",
        "        return regularization\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'parameter': float(tf.keras.backend.get_value(self.parameter))}\n",
        "\n",
        "class CASSI_layer(Layer):\n",
        "    def __init__(self, output_dim=(256, 256, 31), input_dim=(512, 512, 31), compression=1, parm1=0.5, parm2=0.5,\n",
        "                 type_code='Binary_0', trans=0.5, type_reg='Physical', kern=256, noise=40, batch_size=1, HO = (0.25, 0.5, 0.25),shots = 1,decimation = 0.5,train = True, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        #self.shots = int(np.round((compression*(input_dim[0]*input_dim[1]*input_dim[2]))/(input_dim[0]*(input_dim[1]+input_dim[2]-1))))\n",
        "        self.shots = shots\n",
        "        #tf.print(self.shots)\n",
        "\n",
        "        self.parms1 = parm1\n",
        "        self.parms2 = parm2\n",
        "        self.type_code = type_code\n",
        "        self.type_reg = type_reg\n",
        "        self.trans = trans\n",
        "        self.kern = kern\n",
        "        self.noise = noise\n",
        "        self.batch_size = batch_size\n",
        "        self.HO = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(HO,0),0),-1),-1)\n",
        "        self.decimation = decimation\n",
        "        self.train = train\n",
        "\n",
        "        if self.type_code == 'Binary_0':\n",
        "            if self.type_reg =='Physical':\n",
        "                self.my_regularizer = Reg_Binary_0_1(parm1)\n",
        "\n",
        "\n",
        "        super(CASSI_layer, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'output_dim': self.output_dim,\n",
        "            'input_dim': self.input_dim,\n",
        "            'shots': self.shots,\n",
        "            'parms1': self.parms1,\n",
        "            'parms2': self.parms2,\n",
        "            'type_code': self.type_code,\n",
        "            'trans': self.trans,\n",
        "            'kern': self.kern,\n",
        "            'noise': self.noise,\n",
        "            'batch_size': self.batch_size,\n",
        "            'my_regularizer': self.my_regularizer,\n",
        "            'type_reg': self.type_reg,\n",
        "            'HO': self.HO,\n",
        "            'train': self.train})\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        if self.train:\n",
        "          print('Trainable CASSI Layer')\n",
        "          if self.type_code == 'Binary_0':\n",
        "            \n",
        "              H_init = np.random.rand(1, self.kern, self.kern, self.shots)\n",
        "              #H_init = np.random.normal(0, 1, (1, self.kern, self.kern, 1, self.shots)) / np.sqrt(\n",
        "                  #self.kern * self.kern)+0.5\n",
        "              H_init = tf.constant_initializer(H_init)\n",
        "              self.H = self.add_weight(name='H', shape=(1,self.kern, self.kern, 1,self.shots), initializer=H_init, trainable=True,regularizer=self.my_regularizer)\n",
        "        else:\n",
        "          print('No Trainable CASSI Layer')\n",
        "          H_init = tf.constant_initializer(np.random.randint(0,2,size=(1, self.kern, self.kern, self.shots)))\n",
        "          self.H = self.add_weight(name='H', shape=(1,self.kern, self.kern, 1,self.shots), initializer=H_init,trainable=False)\n",
        "  \n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        decimation = self.decimation\n",
        "        K = self.shots\n",
        "        H = self.H\n",
        "        L = self.input_dim[2]\n",
        "        M = int(decimation*self.input_dim[0])\n",
        "        HO = self.HO\n",
        "        # High Order CASSI Sensing Model\n",
        "        inputs = tf.image.resize(inputs,[M,M])\n",
        "        inputs = tf.expand_dims(inputs,-1)\n",
        "        Image2 = tf.reduce_sum(tf.nn.conv3d(inputs,HO,strides=[1,1,1,1,1],padding='SAME'),axis=-1)\n",
        "\n",
        "        Aux1 = tf.multiply(H,tf.expand_dims(Image2,-1))\n",
        "        Aux1 = tf.pad(Aux1, [[0, 0], [0, 0], [0, L - 1], [0, 0],[0, 0]])\n",
        "\n",
        "        Y = None\n",
        "        for i in range(L):\n",
        "            Tempo = tf.roll(Aux1, shift=i, axis=2)\n",
        "            if Y is not None:\n",
        "                Y = tf.concat([Y, tf.expand_dims(Tempo[:, :, :, i], -1)], axis=4)\n",
        "            else:\n",
        "                Y = tf.expand_dims(Tempo[:, :, :, i], -1)\n",
        "        Y = tf.reduce_sum(Y, -1)\n",
        "\n",
        "\n",
        "\n",
        "        # CASSI Transpose model (x = H'*y)\n",
        "\n",
        "        X = None\n",
        "        for i in range(L):\n",
        "            Tempo = tf.roll(Y, shift=-i, axis=2)\n",
        "            if X is not None:\n",
        "                X = tf.concat([X, tf.expand_dims(Tempo[:, :, 0:M], -1)], axis=4)\n",
        "            else:\n",
        "                X = tf.expand_dims(Tempo[:, :, 0:M], -1)\n",
        "\n",
        "\n",
        "        # High Order\n",
        "        X = tf.transpose(X,[0,1,2,4,3])\n",
        "\n",
        "        X2 = None\n",
        "        for i in range(K):\n",
        "          Aux2 = tf.expand_dims(X[:,:,:,:,i],-1)\n",
        "          if X2 is not None:\n",
        "            X2 =tf.concat([X2,tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)],4)\n",
        "          else:\n",
        "            X2 = tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)\n",
        "\n",
        "\n",
        "        X = tf.multiply(H, X)\n",
        "        X = tf.reduce_sum(X,4)\n",
        "\n",
        "        X = X / tf.math.reduce_max(X)\n",
        "\n",
        "\n",
        "        return X,H,Y\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_dim)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONSL5y9nOQCx"
      },
      "source": [
        "# Unrolling Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnppjgxWAGs-"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def TransposeCASSI(yh,H,M,L,shots,decimation):\r\n",
        "  M = int(round(M.numpy()*decimation.numpy()))\r\n",
        "  HO = [0.25,5,0.25]\r\n",
        "  HO = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(HO,0),0),-1),-1)\r\n",
        "  X = None\r\n",
        "  for i in range(L):\r\n",
        "      Tempo = tf.roll(yh, shift=-i, axis=2)\r\n",
        "      if X is not None:\r\n",
        "          X = tf.concat([X, tf.expand_dims(Tempo[:, :, 0:M], -1)], axis=4)\r\n",
        "      else:\r\n",
        "          X = tf.expand_dims(Tempo[:, :, 0:M], -1)\r\n",
        "\r\n",
        "\r\n",
        "  # High Order\r\n",
        "  X = tf.transpose(X,[0,1,2,4,3])\r\n",
        "\r\n",
        "  X2 = None\r\n",
        "  for i in range(shots):\r\n",
        "    Aux2 = tf.expand_dims(X[:,:,:,:,i],-1)\r\n",
        "    if X2 is not None:\r\n",
        "      X2 =tf.concat([X2,tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)],4)\r\n",
        "    else:\r\n",
        "      X2 = tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)\r\n",
        "\r\n",
        "\r\n",
        "  X = tf.multiply(H, X)\r\n",
        "  X = tf.reduce_sum(X,4)\r\n",
        "\r\n",
        "  return X / tf.math.reduce_max(X)\r\n",
        "\r\n",
        "\r\n",
        "def ForwardCASSI(inputs,H,decimation):\r\n",
        "  [B,M,N,L] = inputs.shape\r\n",
        "\r\n",
        "  M = round(M*decimation.numpy())\r\n",
        "\r\n",
        "  M = int(M)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "  HO = [0.25, 0.5, 0.25]\r\n",
        "  HO = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(HO,0),0),-1),-1)\r\n",
        "  # High Order CASSI Sensing Model\r\n",
        "  inputs = tf.image.resize(inputs,[M,M])\r\n",
        "  inputs = tf.expand_dims(inputs,-1)\r\n",
        "  Image2 = tf.reduce_sum(tf.nn.conv3d(inputs,HO,strides=[1,1,1,1,1],padding='SAME'),axis=-1)\r\n",
        "\r\n",
        "  Aux1 = tf.multiply(tf.broadcast_to(H,shape = tf.shape(inputs)),tf.expand_dims(Image2,-1))\r\n",
        "  Aux1 = tf.pad(Aux1, [[0, 0], [0, 0], [0, L - 1], [0, 0],[0, 0]])\r\n",
        "\r\n",
        "  Y = None\r\n",
        "  for i in range(L):\r\n",
        "      Tempo = tf.roll(Aux1, shift=i, axis=2)\r\n",
        "      if Y is not None:\r\n",
        "          Y = tf.concat([Y, tf.expand_dims(Tempo[:, :, :, i], -1)], axis=4)\r\n",
        "      else:\r\n",
        "          Y = tf.expand_dims(Tempo[:, :, :, i], -1)\r\n",
        "  return tf.reduce_sum(Y, -1)\r\n",
        "\r\n",
        "\r\n",
        "def ForwardMultispectral(inputs,H,sd,dm,ds):\r\n",
        "  \r\n",
        "  [B,M,N,L] = inputs.shape\r\n",
        "  \r\n",
        "  Lp = int(round(L*sd.numpy()))\r\n",
        "  inputs = tf.expand_dims(tf.reshape(tf.image.resize(tf.expand_dims(tf.reshape(inputs,[M*N,L]),-1),[M*N,Lp]),[M,N,Lp]),0)\r\n",
        "  y = tf.expand_dims(tf.reduce_sum(tf.multiply(tf.broadcast_to(H,shape= tf.shape(inputs)),inputs),-1),-1)\r\n",
        "\r\n",
        "  \r\n",
        "  return y\r\n",
        "  \r\n",
        "def TransposeMultispectral(y,H,Lp):\r\n",
        "\r\n",
        "  X = None\r\n",
        "  for i in range(Lp):\r\n",
        "    if X is not None:\r\n",
        "\r\n",
        "      X = tf.concat([X,y],-1)\r\n",
        "    else:\r\n",
        "      X = y\r\n",
        "\r\n",
        "  X = tf.transpose(X,[0,1,2,3])\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "  X = tf.multiply(H, X)\r\n",
        "\r\n",
        "\r\n",
        "  return X / tf.math.reduce_max(X)\r\n",
        "\r\n",
        "class Mu_parameter(keras.layers.Layer):\r\n",
        "    def __init__(self, units=1, input_dim=32):\r\n",
        "        super(Mu_parameter, self).__init__()\r\n",
        "        w_init = tf.keras.initializers.Constant(value=0)\r\n",
        "        self.w = tf.Variable(\r\n",
        "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\r\n",
        "            trainable=True, constraint = NonNeg()\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        return tf.multiply(self.w, inputs) \r\n",
        "\r\n",
        "class Lambda_parameter(keras.layers.Layer):\r\n",
        "    def __init__(self, units=1, input_dim=32):\r\n",
        "        super(Lambda_parameter, self).__init__()\r\n",
        "        w_init = tf.keras.initializers.Constant(value=0)\r\n",
        "        self.w = tf.Variable(\r\n",
        "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\r\n",
        "            trainable=True, constraint = NonNeg()\r\n",
        "        )\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        return tf.multiply(self.w, inputs) \r\n",
        "\r\n",
        "\r\n",
        "def GradientMultispectral(x):\r\n",
        "  X = x[0]\r\n",
        "  ym = x[1]\r\n",
        "  Hm = x[2]\r\n",
        "  sd = x[3]\r\n",
        "  dm = x[4]\r\n",
        "  ds = x[5]\r\n",
        "  Lp = x[6]\r\n",
        "\r\n",
        "  \r\n",
        "  ym_e = ForwardMultispectral(X,Hm,sd,dm,ds)\r\n",
        "  rm = ym_e-ym\r\n",
        "  Gm = TransposeMultispectral(rm,Hm,Lp)\r\n",
        "  return Gm\r\n",
        "def GradientCASSI(x):\r\n",
        "  X = x[0]\r\n",
        "  yh = x[1]\r\n",
        "  M = x[2]\r\n",
        "  L = x[3]\r\n",
        "  shots = x[4]\r\n",
        "  decimation = x[5]\r\n",
        "  Hh = x[6]\r\n",
        "  yh_e = ForwardCASSI(X,Hh,decimation)\r\n",
        "  rh = yh_e-yh\r\n",
        "  Gh = TransposeCASSI(rh,Hh,M,L,shots,decimation)\r\n",
        "  return Gh\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOBOQ6h6OYn4"
      },
      "source": [
        "# Unrolling Network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Jxy4bNw5ndS9"
      },
      "source": [
        "\n",
        "def FusionNetwork(pretrained_weights=None, input_size=(512, 512, 31), depth=31, dm = 1, ds = 1, w_ini=400, w_final=700, batch_size=4,decimation = 0.5,sd=0.5,shots=1,trainable=True,parameter = 0.5):\n",
        "  inputs = Input(shape=input_size)\n",
        "\n",
        "  # Customer layers \n",
        "\n",
        "  [Xm,Hm,ym] = MultispectralAcq(output_dim=(input_size[0],input_size[1],1),input_dim=input_size, w_ini=w_ini, w_final=w_final, ds = ds, dm = dm,sd = 0.5,train=trainable)(inputs) \n",
        "\n",
        "  [Xh,Hh,yh] = CASSI_layer(output_dim=(512, 512, 31), input_dim=(512, 512, 31), compression=1, parm1=parameter, parm2=0.5,\n",
        "                 type_code='Binary_0', trans=0.5, type_reg='Physical', kern=256, batch_size=1, HO = (0.25, 0.5, 0.25),shots = 1,decimation=0.5, train =trainable)(inputs)\n",
        "  \n",
        "  #Initialization\n",
        "  #Xh = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(Xh)\n",
        "  Xh = UpSampling2D([int(1/decimation),int(1/decimation)])(Xh)\n",
        "\n",
        "  Xm = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(Xm) \n",
        "  X0 = Add(name='X0')([Xh,Xm])\n",
        "\n",
        "  #Stage 1\n",
        "  stage_1_m = Lambda(GradientMultispectral)([X0,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_1_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_1_m) \n",
        "\n",
        "  stage_1_h = Lambda(GradientCASSI)([X0,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  stage_1_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_1_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X0)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "  \n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X0,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "  \n",
        "  add_x = Subtract()([X0,conv_r5])\n",
        "\n",
        "  PP_1 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G1 = Add()([stage_1_h,stage_1_m,PP_1])\n",
        "\n",
        "  GS_1 = Lambda_parameter()(Add_G1)\n",
        "\n",
        "  X1 = Subtract(name='X1')([X0,GS_1])\n",
        "  \n",
        "  #Stage 2\n",
        "  stage_2_m = Lambda(GradientMultispectral)([X1,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_2_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_2_m) \n",
        "\n",
        "  stage_2_h = Lambda(GradientCASSI)([X1,yh,input_size[0],input_size[2],shots,decimation,Hh])\n",
        "  #stage_2_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_2_h) \n",
        "  stage_2_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_2_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X1)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X1,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X1,conv_r5])\n",
        "  \n",
        "\n",
        "  PP_2 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G2 = Add()([stage_2_h,stage_2_m,PP_2])\n",
        "\n",
        "  GS_2 = Lambda_parameter()(Add_G2)\n",
        "\n",
        "  X2 = Subtract(name='X2')([X1,GS_2])\n",
        "\n",
        "  #Stage 3\n",
        "\n",
        "  stage_3_m = Lambda(GradientMultispectral)([X2,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_3_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_3_m) \n",
        "\n",
        "  stage_3_h = Lambda(GradientCASSI)([X2,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_3_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_3_h) \n",
        "  stage_3_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_3_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X2)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "  \n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X2,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X2,conv_r5])\n",
        "\n",
        "  PP_3 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G3 = Add()([stage_3_h,stage_3_m,PP_3])\n",
        "\n",
        "  GS_3 = Lambda_parameter()(Add_G3)\n",
        "\n",
        "  X3 = Subtract(name='X3')([X2,GS_3])\n",
        "\n",
        "  # Stage 4\n",
        "  stage_4_m = Lambda(GradientMultispectral)([X3,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_4_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_4_m) \n",
        "\n",
        "  stage_4_h = Lambda(GradientCASSI)([X3,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_4_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_4_h) \n",
        "  stage_4_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_4_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X3)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        " \n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X3,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X3,conv_r5]) \n",
        "\n",
        "  PP_4 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G4 = Add()([stage_4_h,stage_4_m,PP_4])\n",
        "\n",
        "  GS_4 = Lambda_parameter()(Add_G4)\n",
        "\n",
        "  X4 = Subtract(name='X4')([X3,GS_4])\n",
        "\n",
        "  # Stage 5\n",
        "  stage_5_m = Lambda(GradientMultispectral)([X4,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_5_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_5_m) \n",
        "\n",
        "  stage_5_h = Lambda(GradientCASSI)([X4,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_5_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_5_h) \n",
        "  stage_5_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_5_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X4)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X4,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X4,conv_r5])\n",
        "\n",
        "  PP_5 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G5 = Add()([stage_5_h,stage_5_m,PP_5])\n",
        "\n",
        "  GS_5 = Lambda_parameter()(Add_G5)\n",
        "\n",
        "  X5 = Subtract(name='X5')([X4,GS_5])\n",
        "  \n",
        "  # Stage 6\n",
        "  stage_6_m = Lambda(GradientMultispectral)([X5,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_6_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_6_m) \n",
        "\n",
        "  stage_6_h = Lambda(GradientCASSI)([X5,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_6_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_6_h) \n",
        "  stage_6_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_6_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X5)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X5,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "  \n",
        "  add_x = Subtract()([X5,conv_r5]) \n",
        "\n",
        "  PP_6 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G6 = Add()([stage_6_h,stage_6_m,PP_6])\n",
        "\n",
        "  GS_6 = Lambda_parameter()(Add_G6)\n",
        "\n",
        "  X6 = Subtract(name='X6')([X5,GS_6])\n",
        "  \n",
        "\n",
        "  # Stage 7\n",
        "\n",
        "\n",
        "  stage_7_m = Lambda(GradientMultispectral)([X6,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_7_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_7_m) \n",
        "\n",
        "  stage_7_h = Lambda(GradientCASSI)([X6,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_7_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_7_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_7_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X6)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "    \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X6,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X6,conv_r5])\n",
        "\n",
        "  PP_7 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G7 = Add()([stage_7_h,stage_7_m,PP_7])\n",
        "\n",
        "  GS_7 = Lambda_parameter()(Add_G7)\n",
        "\n",
        "  X7 = Subtract(name='X7')([X6,GS_7])\n",
        "  \n",
        "  #Stage 8\n",
        "\n",
        "\n",
        "  stage_8_m = Lambda(GradientMultispectral)([X7,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_8_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_8_m) \n",
        "\n",
        "  stage_8_h = Lambda(GradientCASSI)([X7,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_7_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_8_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_8_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X7)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = Add()([X7,conv_r4])\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X7,conv_r5])\n",
        "\n",
        "  PP_8 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G8 = Add()([stage_8_h,stage_8_m,PP_8])\n",
        "\n",
        "  GS_8 = Lambda_parameter()(Add_G8)\n",
        "\n",
        "  X8 = Subtract(name='X8')([X7,GS_8])\n",
        "\n",
        "  #Stage 9\n",
        "\n",
        "  stage_9_m = Lambda(GradientMultispectral)([X8,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_9_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_9_m) \n",
        "\n",
        "  stage_9_h = Lambda(GradientCASSI)([X8,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_9_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_9_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_9_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X8)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X8,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X8,conv_r5])\n",
        "\n",
        "  PP_9 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G9 = Add()([stage_9_h,stage_9_m,PP_9])\n",
        "\n",
        "  GS_9 = Lambda_parameter()(Add_G9)\n",
        "\n",
        "  X9 = Subtract(name='X9')([X8,GS_9])\n",
        " \n",
        "  #Stage 10\n",
        "\n",
        "  stage_10_m = Lambda(GradientMultispectral)([X9,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_10_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_10_m) \n",
        "\n",
        "  stage_10_h = Lambda(GradientCASSI)([X9,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_10_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_10_h) \n",
        "  stage_10_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_10_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X9)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "  \n",
        "  conv_r4 = Add()([X9,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X9,conv_r5])\n",
        "\n",
        "  PP_10 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G10 = Add()([stage_10_h,stage_10_m,PP_10])\n",
        "\n",
        "  GS_10 = Lambda_parameter()(Add_G10)\n",
        "\n",
        "  X10 = Subtract(name='X10')([X9,GS_10])\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  model = Model(inputs,X10)\n",
        "\n",
        "\n",
        "  if (pretrained_weights):\n",
        "      model.load_weights(pretrained_weights)\n",
        "      print('loading weights generator')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsndAI3HOiwA"
      },
      "source": [
        "# Custom Callbacks, Losses and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsdLaKR1Q1XI"
      },
      "source": [
        "\n",
        "def custom_loss():\n",
        "  def lossimage_1(y_true, y_pred):\n",
        "    val = tf.reduce_mean(tf.norm(y_pred - y_true, ord='fro', axis=(1, 2))) + 10*tf.reduce_mean(tf.norm(y_pred - y_true, ord=2, axis=-1))\n",
        "    return val\n",
        "  return lossimage_1\n",
        "\n",
        "\n",
        "\n",
        "def PSNR_Metric(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.image.psnr(y_true,y_pred,1))\n",
        "\n",
        "def ERGAS_Metric(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.keras.losses().update_state(y_true,y_pred))\n",
        "\n",
        "def SSIM_Metric(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.image.ssim(y_pred,y_true,1))\n",
        "\n",
        "def SNR_metric(y_true, y_pred):\n",
        "  return tf.reduce_sum(10*tf.math.log())\n",
        "\n",
        "\n",
        "\n",
        "def custom_loss_3():\n",
        "  def lossimage_3(y_true, y_pred):\n",
        "    val = tf.divide(tf.reduce_sum((y_true-y_pred)**2)**0.5,tf.reduce_sum(y_true)**0.5)\n",
        "    return val\n",
        "  return lossimage_3\n",
        "\n",
        "def custom_loss_2():\n",
        "\n",
        "  def lossimage_2(y_true, y_pred):\n",
        "\n",
        "    \n",
        "    #spatial_loss =  tf.reduce_mean(1-tf.image.ssim(y_pred,y_true,1))\n",
        "    spatial_loss =tf.reduce_mean(tf.norm(y_pred - y_true, ord=1)) + tf.reduce_mean(1-tf.image.ssim_multiscale(y_pred,y_true,1))\n",
        "    #spatial_loss =tf.reduce_mean(tf.norm(y_pred - y_true, ord='fro',axis=(1,2))) + tf.reduce_mean(1-tf.image.ssim(y_pred,y_true,1))\n",
        "    \n",
        "    a_b = tf.math.reduce_sum(tf.multiply(y_pred,y_true),axis=-1)\n",
        "    \n",
        "\n",
        "    mag_a = tf.sqrt(tf.reduce_sum(y_pred**2,axis=-1))\n",
        "    mag_b = tf.sqrt(tf.reduce_sum(y_true**2,axis=-1))\n",
        "\n",
        "    \n",
        "    spectral_loss = tf.reduce_mean(tf.abs(a_b-tf.multiply(mag_a,mag_b))) \n",
        "\n",
        "    spectral_smoothness = tf.reduce_mean(tf.norm(y_pred - y_true, ord=1, axis=3))\n",
        "    \n",
        "\n",
        "    val = 3*spectral_loss + 2*spatial_loss\n",
        "    return val\n",
        "  return lossimage_2\n",
        "\n",
        "\n",
        "class Aument_parameters(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, p_aum,p_step):\n",
        "        super().__init__()\n",
        "        self.p_aum = p_aum\n",
        "        self.p_step = p_step\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        if (tf.math.floormod(epoch,self.p_step)==(self.p_step-1)):\n",
        "            param=self.model.layers[1].my_regularizer.parameter\n",
        "            param = tf.keras.backend.get_value(param)\n",
        "            self.model.layers[1].my_regularizer.parameter.assign(param * self.p_aum)\n",
        "            tf.print(' regularizator ='+ str(param* self.p_aum))\n",
        "    \n",
        "        print(tf.keras.backend.get_value(self.model.layers[1].my_regularizer.parameter))\n",
        "\n",
        "\n",
        "class save_each_epoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, checkpoint_dir):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print('Model Saved at: ' + self.checkpoint_dir)\n",
        "        \n",
        "        self.model.save_weights(self.checkpoint_dir)\n",
        "\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# This is a sample of a scheduler I used in the past\n",
        "def lr_scheduler(epoch, lr):\n",
        "    decay_step = 40\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        lr = lr/2\n",
        "        tf.print(' Learning rate ='+ str(lr))        \n",
        "        return lr\n",
        "    \n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnITTXFhOpla"
      },
      "source": [
        "# Train network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOZdlrFvQwTx"
      },
      "source": [
        " \n",
        "## Run Deep fusion network\n",
        "# load libraries\n",
        " \n",
        " \n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        " \n",
        "# parameters of the net\n",
        "\n",
        " \n",
        "BATCH_SIZE = 1; EPOCHS=300; IMG_WIDTH = 512; IMG_HEIGHT = 512; L_bands = 31; dm=1/9; ds=1; \n",
        " \n",
        "# directory\n",
        "#dataset_path = '/content/drive/Shareddrives/rajc990330@gmail.com/DataFusion'\n",
        "dataset_path = '/content/drive/MyDrive/DeepFusion/data_FUSION'\n",
        "dataset_dict = 'rad'\n",
        "callback_path_fil_train = '/content/drive/MyDrive/DeepFusion/FinalUnrolledNetwork.h5'\n",
        " \n",
        "#---------------------Generator-----------------------------------------------------------------------------------------\n",
        "\n",
        "p_max = 0.5\n",
        "steps = 25\n",
        "p_aum = 10\n",
        "\n",
        "\n",
        "# Transformations \n",
        "D = ImageDataGenerator(rotation_range=180,width_shift_range=0.2, height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        " \n",
        "params = {'dim': (IMG_WIDTH, IMG_HEIGHT, L_bands),\n",
        "          'batch_size': BATCH_SIZE,\n",
        "          'im_path': dataset_path, \n",
        "          'dic_img': dataset_dict,\n",
        "          'shuffle': True, \n",
        "          'augment': False}  # Augmented only for training\n",
        " \n",
        "HyperFiles = [\n",
        "        fn  # Create full paths to images\n",
        "        for fn in listdir(dataset_path)\n",
        "        if isfile(join(dataset_path, fn))\n",
        "           and fn.lower().endswith(('.mat'))\n",
        "    ]\n",
        " \n",
        "print('Training images: ',len(HyperFiles))\n",
        " \n",
        "flag_tr=False\n",
        "train_gen = DataGenerator(HyperFiles, flag_tr, D, **params)\n",
        " \n",
        "params = {'dim': (IMG_WIDTH, IMG_HEIGHT, L_bands),\n",
        "          'batch_size': BATCH_SIZE,\n",
        "          'im_path': dataset_path + '/validation',\n",
        "          'dic_img': dataset_dict,\n",
        "          'shuffle': True,\n",
        "          'augment': False}\n",
        " \n",
        "HyperFiles = [\n",
        "        fn  # Create full paths to images\n",
        "        for fn in listdir(dataset_path + '/validation')\n",
        "        if isfile(join(dataset_path + '/validation', fn))\n",
        "           and fn.lower().endswith(('.mat'))\n",
        "    ]\n",
        " \n",
        "print('Testing images: ',len(HyperFiles))\n",
        " \n",
        "flag_tr=False\n",
        "val_gen = DataGenerator(HyperFiles, flag_tr, D, **params)\n",
        " \n",
        "#-------------Fusion_Net_model------------------------------------------------------------------------------------------\n",
        " \n",
        "'''loss_total = {'output_f1': tf.keras.losses.MeanSquaredError(), \n",
        "              'output_f2': tf.keras.losses.BinaryCrossentropy(from_logits=False)}'''\n",
        "loss_total = custom_loss_2 ()\n",
        "metrics_total = {'output_f1': 'MeanSquaredError'}\n",
        "model = FusionNetwork_2(pretrained_weights=None, input_size=(512, 512, 31), depth=31, batch_size = BATCH_SIZE, dm=dm, ds=ds,decimation=0.5,trainable=True,parameter = 10)\n",
        " \n",
        "model.load_weights(callback_path_fil_train)\n",
        " \n",
        "lossWeights = {'output_f1': 0, 'output_f2': 1}\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=1,\n",
        "    decay_rate=0.5,\n",
        "    staircase=True)\n",
        "\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, amsgrad=True)\n",
        " \n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_total, metrics = [PSNR_Metric]) \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "model.summary()\n",
        " \n",
        " \n",
        " \n",
        "#----------------Run network--------------------------------------------------------------------------------------------\n",
        "cp_callback_train = save_each_epoch(callback_path_fil_train)\n",
        "callbacks = [cp_callback_train,LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "\n",
        "#comment next line just to load the pre-tranined model\n",
        "\n",
        "history = model.fit(x=train_gen, validation_data=val_gen, verbose=1, epochs=EPOCHS, callbacks=[callbacks])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8427PMoIO7YJ"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K4nZZ9D2ps_"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "\n",
        "#Image =scipy.io.loadmat('/content/drive/My Drive/DeepFusion/data_FUSION/Test/BGU_162.mat')['rad'] # Test Image\n",
        "Image =scipy.io.loadmat('/content/drive/Shareddrives/rajc990330@gmail.com/DataFusion/ARAD_HS_117.mat')['rad']\n",
        "# Reconstruction Model\n",
        "\n",
        "def Deep_Model_Test(pretrained_weights=None, input_size = (512,512,31), input_size_mult=(512, 512, 16),input_size_cassi=(256, 256, 31), depth=31, batch_size=4, dm=1, ds=0.5,sd=0.5,decimation=0.5,Hh=None,Hm=None,ym = None,yh = None,shots=1,trainable=True):\n",
        "\n",
        "  # define the model input\n",
        "  input_multi = Input(shape=input_size_mult)\n",
        "  input_cassi = Input(shape=input_size_cassi)\n",
        "  \n",
        "  #Initialization\n",
        "  #Xh = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(Xh)\n",
        "  Xh = UpSampling2D([int(1/decimation),int(1/decimation)])(input_cassi)\n",
        "\n",
        "  Xm = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(input_multi) \n",
        "  X0 = Add()([Xh,Xm])\n",
        "\n",
        "  #Stage 1\n",
        "  stage_1_m = Lambda(GradientMultispectral)([X0,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_1_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_1_m) \n",
        "\n",
        "  stage_1_h = Lambda(GradientCASSI)([X0,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  stage_1_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_1_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X0)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "  \n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X0,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "  \n",
        "  add_x = Subtract()([X0,conv_r5])\n",
        "\n",
        "  PP_1 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G1 = Add()([stage_1_h,stage_1_m,PP_1])\n",
        "\n",
        "  GS_1 = Lambda_parameter()(Add_G1)\n",
        "\n",
        "  X1 = Subtract()([X0,GS_1])\n",
        "  \n",
        "  #Stage 2\n",
        "  stage_2_m = Lambda(GradientMultispectral)([X1,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_2_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_2_m) \n",
        "\n",
        "  stage_2_h = Lambda(GradientCASSI)([X1,yh,input_size[0],input_size[2],shots,decimation,Hh])\n",
        "  #stage_2_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_2_h) \n",
        "  stage_2_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_2_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X1)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X1,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X1,conv_r5])\n",
        "  \n",
        "\n",
        "  PP_2 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G2 = Add()([stage_2_h,stage_2_m,PP_2])\n",
        "\n",
        "  GS_2 = Lambda_parameter()(Add_G2)\n",
        "\n",
        "  X2 = Subtract()([X1,GS_2])\n",
        "\n",
        "  #Stage 3\n",
        "\n",
        "  stage_3_m = Lambda(GradientMultispectral)([X2,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_3_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_3_m) \n",
        "\n",
        "  stage_3_h = Lambda(GradientCASSI)([X2,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_3_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_3_h) \n",
        "  stage_3_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_3_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X2)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "  \n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X2,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X2,conv_r5])\n",
        "\n",
        "  PP_3 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G3 = Add()([stage_3_h,stage_3_m,PP_3])\n",
        "\n",
        "  GS_3 = Lambda_parameter()(Add_G3)\n",
        "\n",
        "  X3 = Subtract()([X2,GS_3])\n",
        "\n",
        "  # Stage 4\n",
        "  stage_4_m = Lambda(GradientMultispectral)([X3,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_4_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_4_m) \n",
        "\n",
        "  stage_4_h = Lambda(GradientCASSI)([X3,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_4_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_4_h) \n",
        "  stage_4_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_4_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X3)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        " \n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X3,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X3,conv_r5]) \n",
        "\n",
        "  PP_4 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G4 = Add()([stage_4_h,stage_4_m,PP_4])\n",
        "\n",
        "  GS_4 = Lambda_parameter()(Add_G4)\n",
        "\n",
        "  X4 = Subtract()([X3,GS_4])\n",
        "\n",
        "  # Stage 5\n",
        "  stage_5_m = Lambda(GradientMultispectral)([X4,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_5_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_5_m) \n",
        "\n",
        "  stage_5_h = Lambda(GradientCASSI)([X4,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_5_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_5_h) \n",
        "  stage_5_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_5_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X4)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X4,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X4,conv_r5])\n",
        "\n",
        "  PP_5 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G5 = Add()([stage_5_h,stage_5_m,PP_5])\n",
        "\n",
        "  GS_5 = Lambda_parameter()(Add_G5)\n",
        "\n",
        "  X5 = Subtract()([X4,GS_5])\n",
        "  \n",
        "  # Stage 6\n",
        "  stage_6_m = Lambda(GradientMultispectral)([X5,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_6_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_6_m) \n",
        "\n",
        "  stage_6_h = Lambda(GradientCASSI)([X5,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_6_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_6_h) \n",
        "  stage_6_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_6_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X5)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X5,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "  \n",
        "  add_x = Subtract()([X5,conv_r5]) \n",
        "\n",
        "  PP_6 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G6 = Add()([stage_6_h,stage_6_m,PP_6])\n",
        "\n",
        "  GS_6 = Lambda_parameter()(Add_G6)\n",
        "\n",
        "  X6 = Subtract()([X5,GS_6])\n",
        "  \n",
        "\n",
        "  # Stage 7\n",
        "\n",
        "\n",
        "  stage_7_m = Lambda(GradientMultispectral)([X6,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_7_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_7_m) \n",
        "\n",
        "  stage_7_h = Lambda(GradientCASSI)([X6,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_7_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_7_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_7_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X6)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "    \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X6,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X6,conv_r5])\n",
        "\n",
        "  PP_7 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G7 = Add()([stage_7_h,stage_7_m,PP_7])\n",
        "\n",
        "  GS_7 = Lambda_parameter()(Add_G7)\n",
        "\n",
        "  X7 = Subtract()([X6,GS_7])\n",
        "  \n",
        "  #Stage 8\n",
        "\n",
        "\n",
        "  stage_8_m = Lambda(GradientMultispectral)([X7,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_8_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_8_m) \n",
        "\n",
        "  stage_8_h = Lambda(GradientCASSI)([X7,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_7_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_8_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_8_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X7)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = Add()([X7,conv_r4])\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X7,conv_r5])\n",
        "\n",
        "  PP_8 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G8 = Add()([stage_8_h,stage_8_m,PP_8])\n",
        "\n",
        "  GS_8 = Lambda_parameter()(Add_G8)\n",
        "\n",
        "  X8 = Subtract()([X7,GS_8])\n",
        "\n",
        "  #Stage 9\n",
        "\n",
        "  stage_9_m = Lambda(GradientMultispectral)([X8,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_9_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_9_m) \n",
        "\n",
        "  stage_9_h = Lambda(GradientCASSI)([X8,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_9_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_7_h) \n",
        "  stage_9_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_9_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X8)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "\n",
        "  conv_r4 = Add()([X8,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X8,conv_r5])\n",
        "\n",
        "  PP_9 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G9 = Add()([stage_9_h,stage_9_m,PP_9])\n",
        "\n",
        "  GS_9 = Lambda_parameter()(Add_G9)\n",
        "\n",
        "  X9 = Subtract()([X8,GS_9])\n",
        " \n",
        "  #Stage 10\n",
        "\n",
        "  stage_10_m = Lambda(GradientMultispectral)([X9,ym,Hm,sd,dm,ds,16]) \n",
        "  stage_10_m = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal',activation=None)(stage_10_m) \n",
        "\n",
        "  stage_10_h = Lambda(GradientCASSI)([X9,yh,input_size[0],input_size[2],shots,decimation,Hh]) \n",
        "  #stage_10_h = Conv2DTranspose(filters=depth, kernel_size=(4,4), strides=(2, 2), padding='SAME', dilation_rate=(4, 4))(stage_10_h) \n",
        "  stage_10_h = UpSampling2D([int(1/decimation),int(1/decimation)])(stage_10_h)\n",
        "\n",
        "  Up1 =  UpSampling2D([2,2])(X9)\n",
        "  \n",
        "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Up1)\n",
        "\n",
        "  conv_r2 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
        "\n",
        "  Down1 = MaxPool2D(pool_size=[2,2])(conv_r2)\n",
        "  \n",
        "  conv_r3 = Conv2D(2*depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(Down1)\n",
        "\n",
        "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(conv_r3)\n",
        "\n",
        "  conv_r4 = BatchNormalization()(conv_r4)\n",
        "  \n",
        "  conv_r4 = Add()([X9,conv_r4])\n",
        "\n",
        "  conv_r5 = Conv2D(depth, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
        "\n",
        "  add_x = Subtract()([X9,conv_r5])\n",
        "\n",
        "  PP_10 = Mu_parameter()(add_x)\n",
        "\n",
        "  Add_G10 = Add()([stage_10_h,stage_10_m,PP_10])\n",
        "\n",
        "  GS_10 = Lambda_parameter()(Add_G10)\n",
        "\n",
        "  X10 = Subtract()([X9,GS_10])\n",
        "  \n",
        "  model = Model([input_cassi,input_multi],X10)\n",
        "\n",
        "  if (pretrained_weights):\n",
        "    model.load_weights(pretrained_weights)\n",
        "    print('loading weights generator')\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Measurements from the trained patterned \n",
        "\n",
        "def GetMeasurements_multi(Hm,inputs,dm,ds,sd=0.5):\n",
        "\n",
        "    \n",
        "    M = 512\n",
        "    N = 512\n",
        "    L = 31\n",
        "    Lp = ceil(sd*L)\n",
        "    inputs = tf.expand_dims(tf.reshape(tf.image.resize(tf.expand_dims(tf.reshape(inputs,[M*N,L]),-1),[M*N,Lp]),[M,N,Lp]),0)\n",
        "\n",
        "    fr,fg,fc,fb=get_color_bases(np.linspace(400, 700, Lp)*1e-9)\n",
        "\n",
        "\n",
        "\n",
        "    wt = Hm[0] + Hm[1] + Hm[2] + Hm[3]\n",
        "    wr = tf.math.divide(Hm[0], wt)\n",
        "    wg = tf.math.divide(Hm[1], wt)\n",
        "    wb = tf.math.divide(Hm[2], wt)\n",
        "    wc = tf.math.divide(Hm[3], wt)\n",
        "\n",
        "    H = tf.multiply(wr, fr) + tf.multiply(wg, fg) + tf.multiply(wb, fb) + tf.multiply(wc, fc)\n",
        "\n",
        "\n",
        "\n",
        "    if ds>dm:\n",
        "      H = tf.keras.backend.repeat_elements(H, rep=int(ceil(1/dm)), axis=2)\n",
        "      H = tf.keras.backend.repeat_elements(H, rep=int(ceil(1/dm)), axis=1)\n",
        "      H = tf.image.resize(H,[M, N])\n",
        "      \n",
        "\n",
        "      y = tf.multiply(H,inputs)\n",
        "      y = tf.expand_dims(tf.reduce_sum(y,axis=-1),axis=-1)\n",
        "    elif dm<ds:\n",
        "      y = tf.expand_dims(tf.reduce_sum(tf.multiply(H,inputs),axis=-1),axis=-1)\n",
        "      y = tf.image.resize(y,[int(M*ds), int(N*ds)])\n",
        "      y = tf.expand_dims(tf.reduce_sum(y,axis=-1),axis=-1)\n",
        "\n",
        "    X = None\n",
        "   \n",
        "\n",
        "\n",
        "   \n",
        "    for i in range(Lp):\n",
        "   \n",
        "\n",
        "      if X is not None:\n",
        "        \n",
        "        X = tf.concat([X,y],-1)        \n",
        "      else:\n",
        "        X = y\n",
        "\n",
        "\n",
        "    X = tf.transpose(X,[0,1,2,3])\n",
        "\n",
        "      \n",
        "      \n",
        "    X = tf.multiply(H, X)\n",
        "\n",
        "\n",
        "    X = X / tf.math.reduce_max(X) \n",
        "\n",
        "    return X,y,H\n",
        "\n",
        "\n",
        "# Measurements from the trained CASSI code aperture \n",
        "\n",
        "\n",
        "def GetMeasurements_cassi(Hh,inputs,shots,HO,decimation=0.5):\n",
        "  L = 31\n",
        "  M = int(decimation*512)\n",
        "\n",
        "  inputs = tf.image.resize(inputs,[M,M])\n",
        "  inputs = tf.expand_dims(inputs,-1)\n",
        "  HO = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(HO,0),0),-1),-1)\n",
        "  K = shots\n",
        "  H = Hh\n",
        "  inputs = tf.cast(inputs,dtype=tf.float32)\n",
        "  # High Order CASSI Sensing Model\n",
        "\n",
        "  inputs = tf.expand_dims(inputs,-1)\n",
        "  Image2 = tf.expand_dims(tf.reduce_sum(tf.nn.conv3d(inputs,HO,strides=[1,1,1,1,1],padding='SAME'),axis=-1),0)\n",
        "\n",
        "  Aux1 = tf.multiply(H,Image2)\n",
        "  Aux1 = tf.pad(Aux1, [[0, 0], [0, 0], [0, L - 1], [0, 0],[0, 0]])\n",
        "\n",
        "  Y = None\n",
        "  for i in range(L):\n",
        "      Tempo = tf.roll(Aux1, shift=i, axis=2)\n",
        "      if Y is not None:\n",
        "          Y = tf.concat([Y, tf.expand_dims(Tempo[:, :, :, i], -1)], axis=4)\n",
        "      else:\n",
        "          Y = tf.expand_dims(Tempo[:, :, :, i], -1)\n",
        "  Y = tf.reduce_sum(Y, -1)\n",
        "  # CASSI Transpose model (x = H'*y)\n",
        "  X = None\n",
        "  for i in range(L):\n",
        "      Tempo = tf.roll(Y, shift=-i, axis=2)\n",
        "      if X is not None:\n",
        "          X = tf.concat([X, tf.expand_dims(Tempo[:, :, 0:M], -1)], axis=4)\n",
        "      else:\n",
        "          X = tf.expand_dims(Tempo[:, :, 0:M], -1)\n",
        "\n",
        "\n",
        "  # High Order\n",
        "  X = tf.transpose(X,[0,1,2,4,3])\n",
        "\n",
        "  X2 = None\n",
        "  for i in range(K):\n",
        "    Aux2 = tf.expand_dims(X[:,:,:,:,i],-1)\n",
        "    if X2 is not None:\n",
        "      X2 =tf.concat([X2,tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)],4)\n",
        "    else:\n",
        "      X2 = tf.expand_dims(tf.reduce_sum(tf.nn.conv3d_transpose(Aux2,HO,[1,M,M,L,1],strides=[1,1,1,1,1],padding='SAME'),axis=-1),-1)\n",
        "\n",
        "\n",
        "  X = tf.multiply(H, X)\n",
        "  X = tf.reduce_sum(X,4)\n",
        "\n",
        "  X = X / tf.math.reduce_max(X)\n",
        "\n",
        "\n",
        "  return X,Y,H\n",
        "\n",
        "Hh = model.get_weights()[0]\n",
        "Hm = model.get_weights()[1:5]\n",
        "\n",
        "\n",
        "[X_m,y_m,H_m] = GetMeasurements_multi(Hm,Image,dm,ds) \n",
        "[X_c,y_c,H_c] = GetMeasurements_cassi(Hh,Image,1,[0.25,0.5,0.25])\n",
        "\n",
        "\n",
        "[M_m,N_m,L_m] = tf.squeeze(X_m).shape\n",
        "[M_c,N_c,L_c] = tf.squeeze(X_c).shape\n",
        "modelTest = Deep_Model_Test(pretrained_weights=None, input_size_mult=(M_m,N_m,L_m),input_size_cassi=(M_c,N_c,L_c), dm=dm, ds=ds, sd=0.5,batch_size = BATCH_SIZE,decimation=0.5,Hh = H_c,Hm =H_m,ym=y_m,yh=y_c)\n",
        "\n",
        "\n",
        "# Assign pretrained weights to the reconstruction model\n",
        "\n",
        "Wrecont = model.get_weights()[5:]\n",
        "\n",
        "\n",
        "\n",
        "modelTest.set_weights(Wrecont)\n",
        "Xr = modelTest.predict([X_c,X_m])\n",
        "\n",
        "PSNR = tf.image.psnr(Image,Xr,1)\n",
        "PSNR = PSNR.numpy()[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjNlkw-PARX"
      },
      "source": [
        "# **Plot results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOlTumAnPuxU"
      },
      "source": [
        "#Visual results\n",
        "plt.gray()\n",
        "plt.imshow(Image[:,:,[15,10,6]]),plt.title('Ground Truth',fontsize=20),plt.show()\n",
        "plt.imshow(np.squeeze(y_m)),plt.title('Measurements Patternerd',fontsize=20),plt.show()\n",
        "plt.imshow(np.squeeze(y_c[0,:,:])),plt.title('Measurements CASSI ' ,fontsize=20),plt.show()\n",
        "plt.imshow(np.squeeze(xr[:,:,:,[15,10,6]])),plt.title('Reconstruction: PSNR = ' + str(PSNR) + '[dB]',fontsize=20),plt.show()\n",
        "\n",
        "#Spectral signature reconstruction. \n",
        "\n",
        "plt.plot(np.squeeze(Image[400,300,:]),label='GT')\n",
        "plt.plot(np.squeeze(X[-1][:,400,300,:]),label='Reconstructed')\n",
        "plt.title('Spectral Signature (400,300)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}