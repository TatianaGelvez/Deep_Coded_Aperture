{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Main_Fusion.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "TvF0CndBFs1m"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvGdsJ0pNojQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaXUgbTnNv-v"
   },
   "source": [
    "# Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z_gmgfEWXD9w"
   },
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import keras\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from keras import backend as K\n",
    "from os import listdir\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.constraints import MinMaxNorm,NonNeg\n",
    "import scipy \n",
    "from scipy import interpolate\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=False)\n",
    "#%cd /content/drive/My Drive/DeepFusion\n",
    "from Read_datasets import *\n",
    "import h5py\n",
    "!nvidia-smi\n",
    "!/usr/local/cuda/bin/nvcc --version\n"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Sat Feb  6 14:43:45 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro K5200        Off  | 00000000:03:00.0  On |                  Off |\r\n",
      "| 31%   51C    P5    32W / 150W |   6828MiB /  8125MiB |      3%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       850      G   /usr/lib/xorg/Xorg                185MiB |\r\n",
      "|    0   N/A  N/A      2159      G   gala                              127MiB |\r\n",
      "|    0   N/A  N/A      2487      G   ...oken=15385009726600882287       41MiB |\r\n",
      "|    0   N/A  N/A      2675      G   ...mviewer/tv_bin/TeamViewer        6MiB |\r\n",
      "|    0   N/A  N/A      2750      G   ...arm-2020.1.2/jbr/bin/java       65MiB |\r\n",
      "|    0   N/A  N/A      2828      G   ...R2020a/bin/glnxa64/MATLAB        1MiB |\r\n",
      "|    0   N/A  N/A      3678      G   ...a/bin/glnxa64/jcef_helper        5MiB |\r\n",
      "|    0   N/A  N/A      3910      C   ...3/envs/env_gpu/bin/python     6381MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\r\n",
      "Built on Thu_Jun_11_22:26:38_PDT_2020\r\n",
      "Cuda compilation tools, release 11.0, V11.0.194\r\n",
      "Build cuda_11.0_bu.TC445_37.28540450_0\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_3m77PGJhPHM"
   },
   "source": [
    "val_dir = '/home/hdsp/Documentos/Bases_datos/ICVL_dataset/val_1/'\n",
    "data_dir = '/home/hdsp/Documentos/Bases_datos/ICVL_dataset/train/'\n",
    "\n",
    "M = 512\n",
    "L = 31\n",
    "batch_size = 2\n",
    "epochs_v = 50\n",
    "p_aum = 1e1\n",
    "p_step = 10\n",
    "\n",
    "train_dataset = Build_data_set(M, M, L, L, batch_size, data_dir)\n",
    "test_dataset = Build_data_set(M, M, L, L, 1, val_dir)\n"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmitancia:  0.4983711946494465\n"
     ]
    }
   ],
   "source": [
    "RGB = [22, 14 , 5]\n",
    "ca_shape = (1 , 512 , 512 + 31 - 1, 1)\n",
    "\n",
    "coded_aperture = np.random.random(ca_shape)\n",
    "\n",
    "coded_aperture = np.asanyarray( (coded_aperture<0.5)*1, dtype=np.float32)\n",
    "ca_total= np.prod(ca_shape)\n",
    "\n",
    "T = np.linalg.norm( coded_aperture.flatten(),1)  / ca_total\n",
    "\n",
    "print('Transmitancia: ', T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from Hx import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONSL5y9nOQCx"
   },
   "source": [
    "# Unrolling Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JnppjgxWAGs-"
   },
   "source": [
    "class Mu_parameter(keras.layers.Layer):\n",
    "    def __init__(self, units=1, input_dim=32):\n",
    "        super(Mu_parameter, self).__init__()\n",
    "        w_init = tf.keras.initializers.Constant(value=0)\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\n",
    "            trainable=True, constraint = NonNeg()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.multiply(self.w, inputs) \n",
    "\n",
    "class Lambda_parameter(keras.layers.Layer):\n",
    "    def __init__(self, units=1, input_dim=32):\n",
    "        super(Lambda_parameter, self).__init__()\n",
    "        w_init = tf.keras.initializers.Constant(value=0)\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\n",
    "            trainable=True, constraint = NonNeg()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.multiply(self.w, inputs) \n",
    "\n",
    "def GradientCASSI(x):\n",
    "  X = x[0]\n",
    "  y = x[1]\n",
    "  H = x[2]\n",
    "  y_e = Forward_D_CASSI(X,H)\n",
    "  rh = y_e-y\n",
    "  Gh = Transpose_D_CASSI(rh,H)\n",
    "  return Gh\n"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOBOQ6h6OYn4"
   },
   "source": [
    "# Unrolling Network\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "code",
    "id": "Jxy4bNw5ndS9"
   },
   "source": [
    "from Custumer_Layers import  DD_CASSI_Layer\n",
    "\n",
    "def HIDDSP(pretrained_weights=None, input_size=(512, 512, 31), depth=64, bands=31):\n",
    "\n",
    "  inputs = Input(shape=input_size,name='image')\n",
    "  X0, y,H = DD_CASSI_Layer(output_dim=input_size, input_dim=input_size,parm1=1e-7)(inputs)\n",
    "  #y = Forward_D_CASSI(inputs,H)\n",
    "  #X0 = Transpose_D_CASSI(y,H)\n",
    "  #--------Stage 1---------------------\n",
    "  # - h step--\n",
    "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X0)\n",
    "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
    "  conv_r1 = Add()([X0,conv_r1])\n",
    "  conv_r1 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X1 = Lambda(GradientCASSI)([X0,y,H])\n",
    "  X1_prior = Subtract()([X0,conv_r1])\n",
    "  X1_prior = Mu_parameter()(X1_prior)\n",
    "\n",
    "  X1 = Add()([X1_prior,X1])\n",
    "  X1 = Lambda_parameter()(X1)\n",
    "  X1 = Subtract(name='X1')([X0,X1])\n",
    "\n",
    "    #--------Stage 2---------------------\n",
    "  # - h step--\n",
    "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X1)\n",
    "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r2)\n",
    "  conv_r2 = Add()([X1,conv_r2])\n",
    "  conv_r2 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r2)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X2 = Lambda(GradientCASSI)([X1,y,H])\n",
    "  X2_prior = Subtract()([X1,conv_r2])\n",
    "  X2_prior = Mu_parameter()(X2_prior)\n",
    "\n",
    "  X2 = Add()([X2_prior,X2])\n",
    "  X2 = Lambda_parameter()(X2)\n",
    "  X2 = Subtract(name='X2')([X1,X2])\n",
    "\n",
    "    #--------Stage 3---------------------\n",
    "  # - h step--\n",
    "  conv_r3 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X2)\n",
    "  conv_r3 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r3)\n",
    "  conv_r3 = Add()([X2,conv_r3])\n",
    "  conv_r3 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r3)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X3 = Lambda(GradientCASSI)([X2,y,H])\n",
    "  X3_prior = Subtract()([X2,conv_r3])\n",
    "  X3_prior = Mu_parameter()(X3_prior)\n",
    "\n",
    "  X3 = Add()([X3_prior,X3])\n",
    "  X3 = Lambda_parameter()(X3)\n",
    "  X3 = Subtract(name='X3')([X2,X3])\n",
    "\n",
    "      #--------Stage 4---------------------\n",
    "  # - h step--\n",
    "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X3)\n",
    "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
    "  conv_r4 = Add()([X3,conv_r4])\n",
    "  conv_r4 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X4 = Lambda(GradientCASSI)([X3,y,H])\n",
    "  X4_prior = Subtract()([X3,conv_r4])\n",
    "  X4_prior = Mu_parameter()(X4_prior)\n",
    "\n",
    "  X4 = Add()([X4_prior,X4])\n",
    "  X4 = Lambda_parameter()(X4)\n",
    "  X4 = Subtract(name='X4')([X3,X4])\n",
    "\n",
    "        #--------Stage 5---------------------\n",
    "  # - h step--\n",
    "  conv_r5 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X4)\n",
    "  conv_r5 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r5)\n",
    "  conv_r5 = Add()([X4,conv_r5])\n",
    "  conv_r5 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r5)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X5 = Lambda(GradientCASSI)([X4,y,H])\n",
    "  X5_prior = Subtract()([X4,conv_r5])\n",
    "  X5_prior = Mu_parameter()(X5_prior)\n",
    "\n",
    "  X5 = Add()([X5_prior,X5])\n",
    "  X5 = Lambda_parameter()(X5)\n",
    "  X5 = Subtract(name='X5')([X4,X5])\n",
    "\n",
    "          #--------Stage 6---------------------\n",
    "  # - h step--\n",
    "  conv_r6 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X5)\n",
    "  conv_r6 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r6)\n",
    "  conv_r6 = Add()([X5,conv_r6])\n",
    "  conv_r6 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r6)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X6 = Lambda(GradientCASSI)([X5,y,H])\n",
    "  X6_prior = Subtract()([X5,conv_r6])\n",
    "  X6_prior = Mu_parameter()(X6_prior)\n",
    "\n",
    "  X6 = Add()([X6_prior,X6])\n",
    "  X6 = Lambda_parameter()(X6)\n",
    "  X6 = Subtract(name='X6')([X5,X6])\n",
    "\n",
    "            #--------Stage 7---------------------\n",
    "  # - h step--\n",
    "  conv_r7 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X6)\n",
    "  conv_r7 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r7)\n",
    "  conv_r7 = Add()([X6,conv_r7])\n",
    "  conv_r7 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r7)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X7 = Lambda(GradientCASSI)([X6,y,H])\n",
    "  X7_prior = Subtract()([X6,conv_r7])\n",
    "  X7_prior = Mu_parameter()(X7_prior)\n",
    "\n",
    "  X7 = Add()([X7_prior,X7])\n",
    "  X7 = Lambda_parameter()(X7)\n",
    "  X7 = Subtract(name='X7')([X6,X7])\n",
    "\n",
    "              #--------Stage 8---------------------\n",
    "  # - h step--\n",
    "  conv_r8 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X7)\n",
    "  conv_r8 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r8)\n",
    "  conv_r8 = Add()([X7,conv_r8])\n",
    "  conv_r8 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r8)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X8 = Lambda(GradientCASSI)([X7,y,H])\n",
    "  X8_prior = Subtract()([X7,conv_r8])\n",
    "  X8_prior = Mu_parameter()(X8_prior)\n",
    "\n",
    "  X8 = Add()([X8_prior,X8])\n",
    "  X8 = Lambda_parameter()(X8)\n",
    "  X8 = Subtract(name='X8')([X7,X8])\n",
    "\n",
    "              #--------Stage 9---------------------\n",
    "  # - h step--\n",
    "  conv_r9 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X8)\n",
    "  conv_r9 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r9)\n",
    "  conv_r9 = Add()([X8,conv_r9])\n",
    "  conv_r9 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r9)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X9 = Lambda(GradientCASSI)([X8,y,H])\n",
    "  X9_prior = Subtract()([X8,conv_r9])\n",
    "  X9_prior = Mu_parameter()(X9_prior)\n",
    "\n",
    "  X9 = Add()([X9_prior,X9])\n",
    "  X9 = Lambda_parameter()(X9)\n",
    "  X9 = Subtract(name='X9')([X8,X9])\n",
    "\n",
    "  model = Model(inputs,X9)\n",
    "\n",
    "\n",
    "  if (pretrained_weights):\n",
    "      model.load_weights(pretrained_weights)\n",
    "      print('loading weights generator')\n",
    "\n",
    "  return model"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsndAI3HOiwA"
   },
   "source": [
    "# Custom Callbacks, Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nsdLaKR1Q1XI"
   },
   "source": [
    "def PSNR_Metric(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.psnr(y_true,y_pred,1))\n",
    "\n",
    "def SSIM_Metric(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.ssim(y_pred,y_true,1))\n",
    "\n",
    "\n",
    "class save_each_epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Model Saved at: ' + self.checkpoint_dir)\n",
    "        \n",
    "        self.model.save_weights(self.checkpoint_dir)\n",
    "\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_step = 40\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        lr = lr/2\n",
    "        tf.print(' Learning rate ='+ str(lr))        \n",
    "        return lr\n",
    "    \n",
    "    return lr"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnITTXFhOpla"
   },
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Mu_parameter.call of <__main__.Mu_parameter object at 0x7f861f0a01d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Mu_parameter.call of <__main__.Mu_parameter object at 0x7f861f0a01d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Lambda_parameter.call of <__main__.Lambda_parameter object at 0x7f861f2d9810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Lambda_parameter.call of <__main__.Lambda_parameter object at 0x7f861f2d9810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "load_weights\n"
     ]
    }
   ],
   "source": [
    "model = HIDDSP(pretrained_weights=None,input_size=(512,512,31), depth=31, bands=31 )\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, amsgrad=True)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics = [PSNR_Metric])\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "check_path = './weights/pp.h5'\n",
    "checkpoint = ModelCheckpoint(check_path, monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, save_weights_only=True ,mode='auto', save_freq='epoch')\n",
    "\n",
    "\n",
    "name = 'Designed_HIDDSP_batch_5_ret_v3'\n",
    "model.load_weights('./weights/'+name+'.h5')\n",
    "print('load_weights')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#from Custumer_Callbacks import Aument_parameters\n",
    "#history = model.fit( train_dataset , epochs=epochs_v, batch_size=batch_size, validation_data=test_dataset, callbacks=[checkpoint,Aument_parameters(p_aum=p_aum, p_step=p_step)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f861e846d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f861e846d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'dd_cassi__layer_3/H:0' shape=(1, 512, 542, 1) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "temporal = model.get_weights()\n",
    "    # obtain the CA\n",
    "CA = temporal[0]\n",
    "scipy.io.savemat('Results/CA_'+name+'.mat', {'CA': CA})\n",
    "\n",
    "for inp,tar in test_dataset.take(-1):\n",
    "    x_test = inp.numpy()\n",
    "    result = model.predict(x_test, batch_size=1)\n",
    "\n",
    "\n",
    "scipy.io.savemat('Results/result_'+name+'.mat', {'result': result})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}