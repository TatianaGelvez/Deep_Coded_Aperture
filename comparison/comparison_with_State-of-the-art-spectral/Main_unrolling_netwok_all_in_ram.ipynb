{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Main_Fusion.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "TvF0CndBFs1m"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvGdsJ0pNojQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaXUgbTnNv-v"
   },
   "source": [
    "# Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z_gmgfEWXD9w"
   },
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import keras\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from keras import backend as K\n",
    "from os import listdir\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.constraints import MinMaxNorm,NonNeg\n",
    "import scipy \n",
    "from scipy import interpolate\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=False)\n",
    "#%cd /content/drive/My Drive/DeepFusion\n",
    "from Filter_pattern import *\n",
    "import h5py\n",
    "!nvidia-smi\n",
    "!/usr/local/cuda/bin/nvcc --version\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Tue Feb  2 10:52:55 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro K5200        Off  | 00000000:03:00.0  On |                  Off |\r\n",
      "| 30%   49C    P0    43W / 150W |    484MiB /  8125MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       827      G   /usr/lib/xorg/Xorg                196MiB |\r\n",
      "|    0   N/A  N/A      1463      G   ...mviewer/tv_bin/TeamViewer        3MiB |\r\n",
      "|    0   N/A  N/A      1624      G   gala                              113MiB |\r\n",
      "|    0   N/A  N/A      1807      G   ...arm-2020.1.2/jbr/bin/java       49MiB |\r\n",
      "|    0   N/A  N/A      2038      G   ...token=4911465998702936853       56MiB |\r\n",
      "|    0   N/A  N/A      2499      C   ...3/envs/env_gpu/bin/python       54MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\r\n",
      "Built on Thu_Jun_11_22:26:38_PDT_2020\r\n",
      "Cuda compilation tools, release 11.0, V11.0.194\r\n",
      "Build cuda_11.0_bu.TC445_37.28540450_0\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_3m77PGJhPHM",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "val_dir = '/home/hdsp/Documentos/Bases_datos/ICVL_dataset/val_1/'\n",
    "data_dir = '/home/hdsp/Documentos/Bases_datos/ICVL_dataset/train/'\n",
    "images_names = [ \"ICVL_\" +str(i+1) + \".mat\" for i in range(0,1530)]\n",
    "val_names =  [ \"ICVL_50\"]\n",
    "\n",
    "\n",
    "dataset_size = (len(images_names),512,512,31)\n",
    "val_size = (len(val_names),512,512,31)\n",
    "\n",
    "Y = np.zeros(dataset_size)\n",
    "Y_val = np.zeros(val_size)\n",
    "i = 0\n",
    "for name in images_names:\n",
    "    Hy = scipy.io.loadmat(data_dir+name)['Hyperimg']\n",
    "    Y[i] = Hy\n",
    "    print(i)\n",
    "    i = i +1\n",
    "\n",
    "print('training')\n",
    "i = 0\n",
    "for name in val_names:\n",
    "    Hy = scipy.io.loadmat(val_dir+name)['Hyperimg']\n",
    "    Y_val[i] = Hy\n",
    "    i = i +1\n",
    "print('val')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "M = 512\n",
    "L = 31\n",
    "batch_size = 2\n",
    "epochs_v = 150\n",
    "p_aum = 1e1\n",
    "p_step = 15\n",
    "RGB = [22, 14 , 5]\n",
    "ca_shape = (1 , 512 , 512 + 31 - 1, 1)\n",
    "\n",
    "coded_aperture = np.random.random(ca_shape)\n",
    "\n",
    "coded_aperture = np.asanyarray( (coded_aperture<0.5)*1, dtype=np.float32)\n",
    "ca_total= np.prod(ca_shape)\n",
    "\n",
    "T = np.linalg.norm( coded_aperture.flatten(),1)  / ca_total\n",
    "\n",
    "print('Transmitancia: ', T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Hx import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONSL5y9nOQCx"
   },
   "source": [
    "# Unrolling Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JnppjgxWAGs-",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class Mu_parameter(keras.layers.Layer):\n",
    "    def __init__(self, units=1, input_dim=32):\n",
    "        super(Mu_parameter, self).__init__()\n",
    "        w_init = tf.keras.initializers.Constant(value=0)\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\n",
    "            trainable=True, constraint = NonNeg()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.multiply(self.w, inputs)\n",
    "\n",
    "class Lambda_parameter(keras.layers.Layer):\n",
    "    def __init__(self, units=1, input_dim=32):\n",
    "        super(Lambda_parameter, self).__init__()\n",
    "        w_init = tf.keras.initializers.Constant(value=0)\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(units,1), dtype=\"float32\"),\n",
    "            trainable=True, constraint = NonNeg()\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.multiply(self.w, inputs)\n",
    "\n",
    "def GradientCASSI(x):\n",
    "  X = x[0]\n",
    "  y = x[1]\n",
    "  H = x[2]\n",
    "  y_e = Forward_D_CASSI(X,H)\n",
    "  rh = y_e-y\n",
    "  Gh = Transpose_D_CASSI(rh,H)\n",
    "  return Gh\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOBOQ6h6OYn4"
   },
   "source": [
    "# Unrolling Network\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "code",
    "id": "Jxy4bNw5ndS9",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "from Custumer_Layers import  DD_CASSI_Layer\n",
    "\n",
    "def HIDDSP(pretrained_weights=None, input_size=(512, 512, 31), depth=64, bands=31):\n",
    "\n",
    "  inputs = Input(shape=input_size,name='image')\n",
    "  X0, y,H = DD_CASSI_Layer(output_dim=input_size, input_dim=input_size,parm1=1e-9)(inputs)\n",
    "  #y = Forward_D_CASSI(inputs,H)\n",
    "  #X0 = Transpose_D_CASSI(y,H)\n",
    "  #--------Stage 1---------------------\n",
    "  # - h step--\n",
    "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X0)\n",
    "  conv_r1 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
    "  conv_r1 = Add()([X0,conv_r1])\n",
    "  conv_r1 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r1)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X1 = Lambda(GradientCASSI)([X0,y,H])\n",
    "  X1_prior = Subtract()([X0,conv_r1])\n",
    "  X1_prior = Mu_parameter()(X1_prior)\n",
    "\n",
    "  X1 = Add()([X1_prior,X1])\n",
    "  X1 = Lambda_parameter()(X1)\n",
    "  X1 = Subtract(name='X1')([X0,X1])\n",
    "\n",
    "    #--------Stage 2---------------------\n",
    "  # - h step--\n",
    "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X1)\n",
    "  conv_r2 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r2)\n",
    "  conv_r2 = Add()([X1,conv_r2])\n",
    "  conv_r2 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r2)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X2 = Lambda(GradientCASSI)([X1,y,H])\n",
    "  X2_prior = Subtract()([X1,conv_r2])\n",
    "  X2_prior = Mu_parameter()(X2_prior)\n",
    "\n",
    "  X2 = Add()([X2_prior,X2])\n",
    "  X2 = Lambda_parameter()(X2)\n",
    "  X2 = Subtract(name='X2')([X1,X2])\n",
    "\n",
    "    #--------Stage 3---------------------\n",
    "  # - h step--\n",
    "  conv_r3 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X2)\n",
    "  conv_r3 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r3)\n",
    "  conv_r3 = Add()([X2,conv_r3])\n",
    "  conv_r3 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r3)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X3 = Lambda(GradientCASSI)([X2,y,H])\n",
    "  X3_prior = Subtract()([X2,conv_r3])\n",
    "  X3_prior = Mu_parameter()(X3_prior)\n",
    "\n",
    "  X3 = Add()([X3_prior,X3])\n",
    "  X3 = Lambda_parameter()(X3)\n",
    "  X3 = Subtract(name='X3')([X2,X3])\n",
    "\n",
    "      #--------Stage 4---------------------\n",
    "  # - h step--\n",
    "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X3)\n",
    "  conv_r4 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
    "  conv_r4 = Add()([X3,conv_r4])\n",
    "  conv_r4 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r4)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X4 = Lambda(GradientCASSI)([X3,y,H])\n",
    "  X4_prior = Subtract()([X3,conv_r4])\n",
    "  X4_prior = Mu_parameter()(X4_prior)\n",
    "\n",
    "  X4 = Add()([X4_prior,X4])\n",
    "  X4 = Lambda_parameter()(X4)\n",
    "  X4 = Subtract(name='X4')([X3,X4])\n",
    "\n",
    "        #--------Stage 5---------------------\n",
    "  # - h step--\n",
    "  conv_r5 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X4)\n",
    "  conv_r5 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r5)\n",
    "  conv_r5 = Add()([X4,conv_r5])\n",
    "  conv_r5 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r5)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X5 = Lambda(GradientCASSI)([X4,y,H])\n",
    "  X5_prior = Subtract()([X4,conv_r5])\n",
    "  X5_prior = Mu_parameter()(X5_prior)\n",
    "\n",
    "  X5 = Add()([X5_prior,X5])\n",
    "  X5 = Lambda_parameter()(X5)\n",
    "  X5 = Subtract(name='X5')([X4,X5])\n",
    "\n",
    "          #--------Stage 6---------------------\n",
    "  # - h step--\n",
    "  conv_r6 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X5)\n",
    "  conv_r6 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r6)\n",
    "  conv_r6 = Add()([X5,conv_r6])\n",
    "  conv_r6 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r6)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X6 = Lambda(GradientCASSI)([X5,y,H])\n",
    "  X6_prior = Subtract()([X5,conv_r6])\n",
    "  X6_prior = Mu_parameter()(X6_prior)\n",
    "\n",
    "  X6 = Add()([X6_prior,X6])\n",
    "  X6 = Lambda_parameter()(X6)\n",
    "  X6 = Subtract(name='X6')([X5,X6])\n",
    "\n",
    "            #--------Stage 7---------------------\n",
    "  # - h step--\n",
    "  conv_r7 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X6)\n",
    "  conv_r7 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r7)\n",
    "  conv_r7 = Add()([X6,conv_r7])\n",
    "  conv_r7 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r7)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X7 = Lambda(GradientCASSI)([X6,y,H])\n",
    "  X7_prior = Subtract()([X6,conv_r7])\n",
    "  X7_prior = Mu_parameter()(X7_prior)\n",
    "\n",
    "  X7 = Add()([X7_prior,X7])\n",
    "  X7 = Lambda_parameter()(X7)\n",
    "  X7 = Subtract(name='X7')([X6,X7])\n",
    "\n",
    "              #--------Stage 8---------------------\n",
    "  # - h step--\n",
    "  conv_r8 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X7)\n",
    "  conv_r8 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r8)\n",
    "  conv_r8 = Add()([X7,conv_r8])\n",
    "  conv_r8 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r8)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X8 = Lambda(GradientCASSI)([X7,y,H])\n",
    "  X8_prior = Subtract()([X7,conv_r8])\n",
    "  X8_prior = Mu_parameter()(X8_prior)\n",
    "\n",
    "  X8 = Add()([X8_prior,X8])\n",
    "  X8 = Lambda_parameter()(X8)\n",
    "  X8 = Subtract(name='X8')([X7,X8])\n",
    "\n",
    "              #--------Stage 9---------------------\n",
    "  # - h step--\n",
    "  conv_r9 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=\"relu\")(X8)\n",
    "  conv_r9 = Conv2D(depth, (3, 3), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r9)\n",
    "  conv_r9 = Add()([X8,conv_r9])\n",
    "  conv_r9 = Conv2D(bands, (1, 1), padding=\"same\", kernel_initializer='he_normal', activation=None)(conv_r9)\n",
    "  # - x step --\n",
    "  # H^T(Hf-y)\n",
    "  X9 = Lambda(GradientCASSI)([X8,y,H])\n",
    "  X9_prior = Subtract()([X8,conv_r9])\n",
    "  X9_prior = Mu_parameter()(X9_prior)\n",
    "\n",
    "  X9 = Add()([X9_prior,X9])\n",
    "  X9 = Lambda_parameter()(X9)\n",
    "  X9 = Subtract(name='X9')([X8,X9])\n",
    "\n",
    "  model = Model(inputs,X9)\n",
    "\n",
    "\n",
    "  if (pretrained_weights):\n",
    "      model.load_weights(pretrained_weights)\n",
    "      print('loading weights generator')\n",
    "\n",
    "  return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsndAI3HOiwA"
   },
   "source": [
    "# Custom Callbacks, Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nsdLaKR1Q1XI",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def PSNR_Metric(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.psnr(y_true,y_pred,1))\n",
    "\n",
    "def SSIM_Metric(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.ssim(y_pred,y_true,1))\n",
    "\n",
    "\n",
    "class save_each_epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Model Saved at: ' + self.checkpoint_dir)\n",
    "\n",
    "        self.model.save_weights(self.checkpoint_dir)\n",
    "\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_step = 40\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        lr = lr/2\n",
    "        tf.print(' Learning rate ='+ str(lr))\n",
    "        return lr\n",
    "\n",
    "    return lr"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnITTXFhOpla"
   },
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HIDDSP(pretrained_weights=None,input_size=(512,512,31), depth=31, bands=31 )\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, amsgrad=True)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics = [PSNR_Metric])\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "check_path = './weights/HIDDSP_1_batch.h5'\n",
    "checkpoint = ModelCheckpoint(check_path, monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, save_weights_only=True ,mode='auto', save_freq='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Custumer_Callbacks import Aument_parameters\n",
    "history = model.fit( x=Y,y=Y , epochs=epochs_v, batch_size=batch_size, validation_data=(Y_val,Y_val), callbacks=[checkpoint,Aument_parameters(p_aum=p_aum, p_step=p_step)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temporal = model.get_weights()\n",
    "    # obtain the CA\n",
    "CA = temporal[0]\n",
    "scipy.io.savemat(\"Results/CA1batch.mat\", {'CA': CA})\n",
    "result = model.predict(Y_val, batch_size=1)\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"Results/result_1batch.mat\", {'result': result})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}